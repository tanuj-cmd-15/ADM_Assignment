{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5-V1U5TMsVT3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import urllib.request\n",
        "import csv\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/groceries.csv\"\n",
        "\n",
        "try:\n",
        "    response = urllib.request.urlopen(url)\n",
        "    dataset = response.read().decode('utf-8')\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXd73wt7gzsA",
        "outputId": "d3b9a9ab-8e94-4daa-ab98-aacd072e769f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the CSV data\n",
        "transactions = []\n",
        "for line in csv.reader(io.StringIO(dataset)):\n",
        "    # Filter out empty strings\n",
        "    transaction = [item for item in line if item.strip()]\n",
        "    if transaction:  # Only add non-empty transactions\n",
        "        transactions.append(transaction)\n",
        "\n",
        "print(f\"Total number of transactions: {len(transactions)}\")\n",
        "print(\"\\nSample transactions:\")\n",
        "for i in range(min(5, len(transactions))):\n",
        "    print(transactions[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxbC5wdHg_FU",
        "outputId": "8a9d8d27-62d3-4b91-9f03-fe95cda38880"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of transactions: 9835\n",
            "\n",
            "Sample transactions:\n",
            "['citrus fruit', 'semi-finished bread', 'margarine', 'ready soups']\n",
            "['tropical fruit', 'yogurt', 'coffee']\n",
            "['whole milk']\n",
            "['pip fruit', 'yogurt', 'cream cheese ', 'meat spreads']\n",
            "['other vegetables', 'whole milk', 'condensed milk', 'long life bakery product']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ManualApriori:\n",
        "    def __init__(self, min_support=0.01, min_confidence=0.6):\n",
        "        self.min_support = min_support\n",
        "        self.min_confidence = min_confidence\n",
        "        self.frequent_itemsets = {}\n",
        "        self.transaction_count = 0\n",
        "        self.item_counts = {}\n",
        "\n",
        "    def _init_pass(self, transactions):\n",
        "        \"\"\"Generate C1: candidate 1-itemsets\"\"\"\n",
        "        self.transaction_count = len(transactions)\n",
        "        C1 = {}\n",
        "        for transaction in transactions:\n",
        "            for item in set(transaction):\n",
        "                if item in C1:\n",
        "                    C1[item] += 1\n",
        "                else:\n",
        "                    C1[item] = 1\n",
        "        return C1\n",
        "\n",
        "    def _is_frequent(self, itemset_count):\n",
        "        \"\"\"Check if the itemset meets minimum support\"\"\"\n",
        "        return itemset_count / self.transaction_count >= self.min_support\n",
        "\n",
        "    def _candidate_gen(self, Fk_1):\n",
        "        \"\"\"Generate candidate k-itemsets from frequent (k-1)-itemsets\"\"\"\n",
        "        Ck = {}\n",
        "        Fk_1_list = list(Fk_1.keys())\n",
        "        k = len(Fk_1_list[0]) + 1 if Fk_1_list else 1\n",
        "\n",
        "        for i in range(len(Fk_1_list)):\n",
        "            for j in range(i+1, len(Fk_1_list)):\n",
        "                items1 = sorted(Fk_1_list[i])\n",
        "                items2 = sorted(Fk_1_list[j])\n",
        "\n",
        "                if k > 2 and items1[:-1] != items2[:-1]:\n",
        "                    continue\n",
        "\n",
        "                new_candidate = tuple(sorted(set(items1) | set(items2)))\n",
        "                if len(new_candidate) == k:\n",
        "                    Ck[new_candidate] = 0\n",
        "\n",
        "        Ck_pruned = Ck.copy()\n",
        "        for candidate in Ck:\n",
        "            for i in range(k):\n",
        "                subset = tuple(sorted(list(candidate)[:i] + list(candidate)[i+1:]))\n",
        "                if subset not in Fk_1:\n",
        "                    if candidate in Ck_pruned:\n",
        "                        del Ck_pruned[candidate]\n",
        "                    break\n",
        "\n",
        "        return Ck_pruned\n",
        "\n",
        "    def find_frequent_itemsets(self, transactions):\n",
        "        \"\"\"Find all frequent itemsets using the Apriori algorithm\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "\n",
        "        C1 = self._init_pass(transactions)\n",
        "\n",
        "        self.item_counts = {(item,): count for item, count in C1.items()}\n",
        "\n",
        "\n",
        "        F1 = {(item,): count for item, count in C1.items() if self._is_frequent(count)}\n",
        "        self.frequent_itemsets[1] = F1\n",
        "\n",
        "        print(f\"Found {len(F1)} frequent 1-itemsets\")\n",
        "\n",
        "        k = 2\n",
        "        while self.frequent_itemsets[k-1]:\n",
        "\n",
        "            Ck = self._candidate_gen(self.frequent_itemsets[k-1])\n",
        "\n",
        "            print(f\"Generated {len(Ck)} candidate {k}-itemsets\")\n",
        "\n",
        "            for transaction in transactions:\n",
        "                transaction_set = set(transaction)\n",
        "                for candidate in Ck:\n",
        "                    if set(candidate).issubset(transaction_set):\n",
        "                        Ck[candidate] += 1\n",
        "\n",
        "            Fk = {candidate: count for candidate, count in Ck.items()\n",
        "                  if self._is_frequent(count)}\n",
        "\n",
        "            if Fk:\n",
        "                self.frequent_itemsets[k] = Fk\n",
        "                print(f\"Found {len(Fk)} frequent {k}-itemsets\")\n",
        "                k += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "            print(f\"Iteration {k-1} completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "        total_frequent_itemsets = sum(len(itemsets) for itemsets in self.frequent_itemsets.values())\n",
        "        print(f\"Total frequent itemsets found: {total_frequent_itemsets}\")\n",
        "\n",
        "        return self.frequent_itemsets\n",
        "\n",
        "    def _get_all_subsets(self, itemset):\n",
        "        \"\"\"Get all non-empty proper subsets of an itemset\"\"\"\n",
        "        n = len(itemset)\n",
        "        subsets = []\n",
        "        for i in range(1, 2**n - 1):\n",
        "            subset = tuple(sorted(itemset[j] for j in range(n) if (i & (1 << j))))\n",
        "            subsets.append(subset)\n",
        "        return subsets\n",
        "\n",
        "    def generate_rules(self):\n",
        "        \"\"\"Generate association rules from frequent itemsets\"\"\"\n",
        "        print(\"\\nGenerating association rules...\")\n",
        "        rules = []\n",
        "        rule_count = 0\n",
        "\n",
        "        for k in range(2, len(self.frequent_itemsets) + 1):\n",
        "            for itemset, itemset_count in self.frequent_itemsets[k].items():\n",
        "                itemset_support = itemset_count / self.transaction_count\n",
        "\n",
        "                subsets = self._get_all_subsets(itemset)\n",
        "\n",
        "                for antecedent in subsets:\n",
        "                    consequent = tuple(sorted(set(itemset) - set(antecedent)))\n",
        "\n",
        "                    if not consequent:\n",
        "                        continue\n",
        "\n",
        "                    ant_support_count = 0\n",
        "                    for size, itemsets in self.frequent_itemsets.items():\n",
        "                        if size == len(antecedent) and antecedent in itemsets:\n",
        "                            ant_support_count = itemsets[antecedent]\n",
        "                            break\n",
        "\n",
        "                    ant_support = ant_support_count / self.transaction_count\n",
        "\n",
        "                    confidence = itemset_support / ant_support\n",
        "\n",
        "                    if confidence >= self.min_confidence:\n",
        "                        cons_support_count = 0\n",
        "                        for size, itemsets in self.frequent_itemsets.items():\n",
        "                            if size == len(consequent) and consequent in itemsets:\n",
        "                                cons_support_count = itemsets[consequent]\n",
        "                                break\n",
        "\n",
        "                        cons_support = cons_support_count / self.transaction_count\n",
        "                        lift = confidence / cons_support\n",
        "\n",
        "                        rules.append({\n",
        "                            'antecedent': antecedent,\n",
        "                            'consequent': consequent,\n",
        "                            'support': itemset_support,\n",
        "                            'confidence': confidence,\n",
        "                            'lift': lift\n",
        "                        })\n",
        "                        rule_count += 1\n",
        "\n",
        "        print(f\"Total rules generated: {len(rules)}\")\n",
        "        return rules\n"
      ],
      "metadata": {
        "id": "_4HizuObhYar"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning manual Apriori implementation...\")\n",
        "start_time = time.time()\n",
        "apriori_manual = ManualApriori(min_support=0.01, min_confidence=0.50)\n",
        "frequent_itemsets = apriori_manual.find_frequent_itemsets(transactions)\n",
        "total_time = time.time() - start_time\n",
        "print(f\"Frequent itemset generation completed in {total_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OQeD7fdh2T7",
        "outputId": "3d8ce33e-b09c-45ad-b8c2-6e164c8f6bea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running manual Apriori implementation...\n",
            "Found 88 frequent 1-itemsets\n",
            "Generated 3828 candidate 2-itemsets\n",
            "Found 213 frequent 2-itemsets\n",
            "Iteration 2 completed in 7.48 seconds\n",
            "Generated 576 candidate 3-itemsets\n",
            "Found 32 frequent 3-itemsets\n",
            "Iteration 3 completed in 8.70 seconds\n",
            "Generated 6 candidate 4-itemsets\n",
            "Total frequent itemsets found: 333\n",
            "Frequent itemset generation completed in 8.72 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "rules = apriori_manual.generate_rules()\n",
        "rule_gen_time = time.time() - start_time\n",
        "print(f\"Rule generation completed in {rule_gen_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKnr3isHh3tb",
        "outputId": "7b43db69-9982-4413-bd22-7daa90e27f8b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating association rules...\n",
            "Total rules generated: 15\n",
            "Rule generation completed in 0.00 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSample frequent itemsets (support count):\")\n",
        "for k in range(1, min(4, len(frequent_itemsets) + 1)):\n",
        "    print(f\"\\n{k}-itemsets (showing up to 5):\")\n",
        "    for idx, (items, count) in enumerate(list(frequent_itemsets[k].items())[:5]):\n",
        "        print(f\"  {items}: {count} (support: {count/apriori_manual.transaction_count:.4f})\")\n",
        "    if len(frequent_itemsets[k]) > 5:\n",
        "        print(f\"  ... and {len(frequent_itemsets[k]) - 5} more\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vle3SBH_h-3Q",
        "outputId": "cc3e8f2c-8b4b-44e0-e7bb-1c3c8f32d718"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample frequent itemsets (support count):\n",
            "\n",
            "1-itemsets (showing up to 5):\n",
            "  ('semi-finished bread',): 174 (support: 0.0177)\n",
            "  ('margarine',): 576 (support: 0.0586)\n",
            "  ('citrus fruit',): 814 (support: 0.0828)\n",
            "  ('coffee',): 571 (support: 0.0581)\n",
            "  ('yogurt',): 1372 (support: 0.1395)\n",
            "  ... and 83 more\n",
            "\n",
            "2-itemsets (showing up to 5):\n",
            "  ('margarine', 'yogurt'): 140 (support: 0.0142)\n",
            "  ('margarine', 'whole milk'): 238 (support: 0.0242)\n",
            "  ('margarine', 'other vegetables'): 194 (support: 0.0197)\n",
            "  ('margarine', 'rolls/buns'): 145 (support: 0.0147)\n",
            "  ('bottled water', 'margarine'): 101 (support: 0.0103)\n",
            "  ... and 208 more\n",
            "\n",
            "3-itemsets (showing up to 5):\n",
            "  ('citrus fruit', 'whole milk', 'yogurt'): 101 (support: 0.0103)\n",
            "  ('citrus fruit', 'other vegetables', 'whole milk'): 128 (support: 0.0130)\n",
            "  ('citrus fruit', 'other vegetables', 'root vegetables'): 102 (support: 0.0104)\n",
            "  ('tropical fruit', 'whole milk', 'yogurt'): 149 (support: 0.0151)\n",
            "  ('other vegetables', 'tropical fruit', 'yogurt'): 121 (support: 0.0123)\n",
            "  ... and 27 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTop 5 association rules by confidence:\")\n",
        "sorted_rules = sorted(rules, key=lambda x: x['confidence'], reverse=True)\n",
        "for idx, rule in enumerate(sorted_rules[:10]):\n",
        "    antecedent_str = ', '.join([f'\"{item}\"' for item in rule['antecedent']])\n",
        "    consequent_str = ', '.join([f'\"{item}\"' for item in rule['consequent']])\n",
        "    print(f\"{idx+1}. {{{antecedent_str}}} => {{{consequent_str}}}\")\n",
        "    print(f\"   Support: {rule['support']:.4f}, Confidence: {rule['confidence']:.4f}, Lift: {rule['lift']:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgfiOfgviH9K",
        "outputId": "87231586-b9ee-4f0e-c97d-1d6f38f5d987"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 association rules by confidence:\n",
            "1. {\"citrus fruit\", \"root vegetables\"} => {\"other vegetables\"}\n",
            "   Support: 0.0104, Confidence: 0.5862, Lift: 3.0296\n",
            "2. {\"root vegetables\", \"tropical fruit\"} => {\"other vegetables\"}\n",
            "   Support: 0.0123, Confidence: 0.5845, Lift: 3.0210\n",
            "3. {\"curd\", \"yogurt\"} => {\"whole milk\"}\n",
            "   Support: 0.0101, Confidence: 0.5824, Lift: 2.2791\n",
            "4. {\"butter\", \"other vegetables\"} => {\"whole milk\"}\n",
            "   Support: 0.0115, Confidence: 0.5736, Lift: 2.2449\n",
            "5. {\"root vegetables\", \"tropical fruit\"} => {\"whole milk\"}\n",
            "   Support: 0.0120, Confidence: 0.5700, Lift: 2.2310\n",
            "6. {\"root vegetables\", \"yogurt\"} => {\"whole milk\"}\n",
            "   Support: 0.0145, Confidence: 0.5630, Lift: 2.2034\n",
            "7. {\"domestic eggs\", \"other vegetables\"} => {\"whole milk\"}\n",
            "   Support: 0.0123, Confidence: 0.5525, Lift: 2.1623\n",
            "8. {\"whipped/sour cream\", \"yogurt\"} => {\"whole milk\"}\n",
            "   Support: 0.0109, Confidence: 0.5245, Lift: 2.0527\n",
            "9. {\"rolls/buns\", \"root vegetables\"} => {\"whole milk\"}\n",
            "   Support: 0.0127, Confidence: 0.5230, Lift: 2.0469\n",
            "10. {\"other vegetables\", \"pip fruit\"} => {\"whole milk\"}\n",
            "   Support: 0.0135, Confidence: 0.5175, Lift: 2.0254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LRhpKzVngswy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}